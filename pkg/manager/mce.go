package manager

import (
	"context"
	"encoding/base64"
	"fmt"
	"time"

	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"sigs.k8s.io/controller-runtime/pkg/client"

	"github.com/ghodss/yaml"
	"github.com/openshift/ci-chat-bot/pkg/utils"
	hivev1 "github.com/openshift/hive/apis/hive/v1"
	"github.com/openshift/installer/pkg/ipnet"
	installer "github.com/openshift/installer/pkg/types"
	"github.com/openshift/installer/pkg/types/aws"
	clusterv1 "open-cluster-management.io/api/cluster/v1"

	hiveaws "github.com/openshift/hive/apis/hive/v1/aws"
)

func (m *jobManager) createManagedCluster(providedImageSet, platform, user, slackChannel string) (*clusterv1.ManagedCluster, error) {
	m.mceConfig.Mutex.RLock()
	mceUserConfig := m.mceConfig.Users[user]
	m.mceConfig.Mutex.RUnlock()
	clusterName, err := generateRandomString(15, true)
	if err != nil {
		return nil, fmt.Errorf("Failed to generate random name: %v", err)
	}
	clusterName = fmt.Sprintf("chat-bot-%s", clusterName)

	// All managed clusters get their own namespace. The namespace is automatically deleted when the ManagedCluster within it is deleted.
	if _, err := m.dpcrNamespaceClient.Create(context.TODO(), &v1.Namespace{ObjectMeta: metav1.ObjectMeta{Name: clusterName, Labels: map[string]string{utils.LaunchLabel: "true"}}}, metav1.CreateOptions{}); err != nil {
		return nil, fmt.Errorf("failed to create namespace: %w", err)
	}

	// copy credentials from main chat-bot secrets namespace
	chatBotSecretsClient := m.dpcrCoreClient.Secrets("ci-chat-bot-mce-config")
	clusterSecretsClient := m.dpcrCoreClient.Secrets(clusterName)
	credentialsName := fmt.Sprintf("%s-credentials", platform)
	if platform == "aws" && mceUserConfig.AwsSecret != "" {
		credentialsName = m.mceConfig.Users[user].AwsSecret
	} else if platform == "gcp" && mceUserConfig.GcpSecret != "" {
		credentialsName = m.mceConfig.Users[user].GcpSecret
	}
	platformsCreds, err := chatBotSecretsClient.Get(context.TODO(), credentialsName, metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get platform (%s) credentials: %v", platform, err)
	}
	platformsCreds.SetNamespace(clusterName)
	platformsCreds.SetName(fmt.Sprintf("%s-%s-creds", clusterName, platform))
	if _, err := clusterSecretsClient.Create(context.TODO(), platformsCreds, metav1.CreateOptions{}); err != nil {
		return nil, fmt.Errorf("failed to create platform (%s) credentials: %v", platform, err)
	}
	pullSecret, err := chatBotSecretsClient.Get(context.TODO(), "mce-pull-secret", metav1.GetOptions{})
	if err != nil {
		return nil, fmt.Errorf("failed to get pull secret: %v", err)
	}
	pullSecret.SetNamespace(clusterName)
	pullSecret.SetName(fmt.Sprintf("%s-pull-secret", clusterName))
	if _, err := clusterSecretsClient.Create(context.TODO(), pullSecret, metav1.CreateOptions{}); err != nil {
		return nil, fmt.Errorf("failed to create pull secret: %v", err)
	}
	baseDomain := fmt.Sprintf("crt-mce-%s.devcluster.openshift.com", platform)
	if platform == "aws" && mceUserConfig.AwsBaseDomain != "" {
		baseDomain = mceUserConfig.AwsBaseDomain
	} else if platform == "gcp" && mceUserConfig.GcpBaseDomain != "" {
		baseDomain = mceUserConfig.GcpBaseDomain
	}
	replicas := int64(3)
	// copied from autogenerated installConfig from web interface
	installConfig := &installer.InstallConfig{
		ObjectMeta: metav1.ObjectMeta{
			Name: clusterName,
		},
		BaseDomain: baseDomain,
		ControlPlane: &installer.MachinePool{
			Hyperthreading: "Enabled",
			Name:           "master",
			Replicas:       &replicas,
			Platform: installer.MachinePoolPlatform{
				AWS: &aws.MachinePool{
					Zones: []string{"us-east-1a"},
					EC2RootVolume: aws.EC2RootVolume{
						IOPS: 4000,
						Size: 100,
						Type: "io1",
					},
					InstanceType: "m5.xlarge",
				},
			},
		},
		Compute: []installer.MachinePool{{
			Hyperthreading: "Enabled",
			Name:           "compute",
			Replicas:       &replicas,
			Platform: installer.MachinePoolPlatform{
				AWS: &aws.MachinePool{
					Zones: []string{"us-east-1a"},
					EC2RootVolume: aws.EC2RootVolume{
						IOPS: 2000,
						Size: 100,
						Type: "io1",
					},
					InstanceType: "m5.xlarge",
				},
			},
		}},
		Networking: &installer.Networking{
			NetworkType: "OVNKubernetes",
			ClusterNetwork: []installer.ClusterNetworkEntry{
				{
					CIDR:       *ipnet.MustParseCIDR("10.128.0.0/14"),
					HostPrefix: 23,
				},
			},
			MachineNetwork: []installer.MachineNetworkEntry{
				{
					CIDR: *ipnet.MustParseCIDR("10.0.0.0/16"),
				},
			},
			ServiceNetwork: []ipnet.IPNet{*ipnet.MustParseCIDR("172.30.0.0/16")},
		},
		Platform: installer.Platform{
			AWS: &aws.Platform{
				Region: "us-east-1",
			},
		},
		PullSecret: "", // skip, hive will inject based on it's secrets
	}
	installConfigBytes, err := yaml.Marshal(installConfig)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal install config: %v", err)
	}
	installConfigSecret := v1.Secret{
		ObjectMeta: metav1.ObjectMeta{
			Name:      fmt.Sprintf("%s-install-config", clusterName),
			Namespace: clusterName,
		},
		Type: v1.SecretTypeOpaque,
		Data: map[string][]byte{
			"install-config.yaml": []byte(base64.StdEncoding.EncodeToString(installConfigBytes)),
		},
	}
	if _, err := clusterSecretsClient.Create(context.TODO(), &installConfigSecret, metav1.CreateOptions{}); err != nil {
		return nil, fmt.Errorf("failed to create install config secret: %v", err)
	}

	expiryTime := time.Now().Add(3 * (time.Hour)).Format(time.RFC3339)
	managedCluster := clusterv1.ManagedCluster{
		ObjectMeta: metav1.ObjectMeta{
			Name: clusterName,
			Labels: map[string]string{
				"Cloud":  "Amazon",
				"Region": "us-east-1",
				"name":   clusterName,
				"vendor": "OpenShift",
				"cluster.open-cluster-management.io/clusterset": "ci-chat-bot",
				utils.LaunchLabel:    "true",
				utils.UserTag:        user,
				utils.ChannelTag:     slackChannel,
				utils.ExpiryTimeTag:  base64.RawStdEncoding.EncodeToString([]byte(expiryTime)),
				utils.RequestTimeTag: base64.RawStdEncoding.EncodeToString([]byte(time.Now().Format(time.RFC3339))),
			},
		},
		Spec: clusterv1.ManagedClusterSpec{
			HubAcceptsClient: true,
		},
	}

	if err := m.dpcrOcmClient.Create(context.TODO(), &managedCluster, &client.CreateOptions{}); err != nil {
		return nil, fmt.Errorf("failed to create managed cluster object: %v", err)
	}

	attemptLimit := int32(1)
	clusterDeployment := hivev1.ClusterDeployment{
		ObjectMeta: metav1.ObjectMeta{
			Name:      clusterName,
			Namespace: clusterName,
			Labels: map[string]string{
				"cloud":  "AWS",
				"region": "us-east-1",
				"vendor": "OpenShift",
				"cluster.open-cluster-management.io/clusterset": "ci-chat-bot",
			},
		},
		Spec: hivev1.ClusterDeploymentSpec{
			BaseDomain:  baseDomain,
			ClusterName: clusterName,
			ControlPlaneConfig: hivev1.ControlPlaneConfigSpec{
				ServingCertificates: hivev1.ControlPlaneServingCertificateSpec{},
			},
			InstallAttemptsLimit: &attemptLimit,
			Installed:            false,
			Platform: hivev1.Platform{
				AWS: &hiveaws.Platform{
					CredentialsSecretRef: v1.LocalObjectReference{
						Name: fmt.Sprintf("%s-%s-creds", clusterName, platform),
					},
					Region: "us-east-1",
				},
			},
			Provisioning: &hivev1.Provisioning{
				InstallConfigSecretRef: &v1.LocalObjectReference{
					Name: fmt.Sprintf("%s-install-config", clusterName),
				},
				ImageSetRef: &hivev1.ClusterImageSetReference{
					Name: providedImageSet,
				},
			},
			PullSecretRef: &v1.LocalObjectReference{
				Name: fmt.Sprintf("%s-pull-secret", clusterName),
			},
		},
	}

	if err := m.dpcrHiveClient.Create(context.TODO(), &clusterDeployment, &client.CreateOptions{}); err != nil {
		return nil, fmt.Errorf("failed to create cluster deployment: %v", err)
	}

	return &managedCluster, nil
}

func (m *jobManager) listManagedClusters() ([]*clusterv1.ManagedCluster, []*hivev1.ClusterDeployment, error) {
	namespaces, err := m.dpcrNamespaceClient.List(context.TODO(), metav1.ListOptions{LabelSelector: utils.LaunchLabel})
	if err != nil {
		return nil, nil, fmt.Errorf("Failed to get list of managed clusters: %v", err)
	}
	managedClusters := []*clusterv1.ManagedCluster{}
	clusterDeployments := []*hivev1.ClusterDeployment{}
	for _, namespace := range namespaces.Items {
		cluster := clusterv1.ManagedCluster{}
		if err := m.dpcrOcmClient.Get(context.TODO(), client.ObjectKey{Namespace: namespace.Name, Name: namespace.Name}, &cluster); err != nil {
			return nil, nil, fmt.Errorf("Failed to get managed cluster for %s namespace: %v", namespace.Name, err)
		}
		managedClusters = append(managedClusters, &cluster)
		clusterDeployment := hivev1.ClusterDeployment{}
		if err := m.dpcrHiveClient.Get(context.TODO(), client.ObjectKey{Namespace: namespace.Name, Name: namespace.Name}, &clusterDeployment); err != nil {
			return nil, nil, fmt.Errorf("Failed to get cluster deployment for %s namespace: %v", namespace.Name, err)
		}
		clusterDeployments = append(clusterDeployments, &clusterDeployment)
	}
	return managedClusters, clusterDeployments, nil
}

func (m *jobManager) printManagedClusters() (string, error) {
	managedClusters, _, err := m.listManagedClusters()
	if err != nil {
		return "", err
	}
	response := fmt.Sprintf("There are %d MCE clusters managed by ci-chat-bot:", len(managedClusters))
	for _, cluster := range managedClusters {
		response += fmt.Sprintf("\n%s: %s", cluster.Name, cluster.Status.Version)
	}
	return response, nil
}

func (m *jobManager) cleanupExpiredClusters() error {
	clusters, _, err := m.listManagedClusters()
	if err != nil {
		return err
	}
	for _, cluster := range clusters {
		expiryString := cluster.Labels["expiry-time"]
		expiryTime, err := time.Parse(time.RFC3339, expiryString)
		if err != nil {
			return err
		}
		if time.Now().Compare(expiryTime) > 0 {
			if err := m.dpcrOcmClient.Delete(context.TODO(), cluster); err != nil {
				return err
			}
		}
	}
	return nil
}

// getClusterAuth returns kubeconfig,password,error
func (m *jobManager) getClusterAuth(name string) (string, string, error) {
	m.mceClusters.lock.RLock()
	{
		defer m.mceClusters.lock.RUnlock()
		// don't pull clusterdeployment if auth is already cached
		kubeconfig, ok := m.mceClusters.clusterKubeconfigs[name]
		password, ok2 := m.mceClusters.clusterPasswords[name]
		if ok && ok2 {
			return kubeconfig, password, nil
		}
	}
	clusterDeployment := hivev1.ClusterDeployment{}
	if err := m.dpcrHiveClient.Get(context.TODO(), client.ObjectKey{Namespace: name, Name: name}, &clusterDeployment); err != nil {
		return "", "", err
	}
	secretsClient := m.dpcrCoreClient.Secrets(name)
	kubeconfigSecret, err := secretsClient.Get(context.TODO(), clusterDeployment.Spec.ClusterMetadata.AdminKubeconfigSecretRef.Name, metav1.GetOptions{})
	if err != nil {
		return "", "", err
	}
	kubeconfig, err := base64.RawStdEncoding.DecodeString(string(kubeconfigSecret.Data["kubeconfig"]))
	if err != nil {
		return "", "", fmt.Errorf("failed to decode kubeconfig: %v", err)
	}
	passwordSecret, err := secretsClient.Get(context.TODO(), clusterDeployment.Spec.ClusterMetadata.AdminPasswordSecretRef.Name, metav1.GetOptions{})
	if err != nil {
		return "", "", err
	}
	password, err := base64.RawStdEncoding.DecodeString(string(passwordSecret.Data["password"]))
	if err != nil {
		return "", "", fmt.Errorf("failed to decode password: %v", err)
	}
	m.mceClusters.lock.Lock()
	defer m.mceClusters.lock.Unlock()
	m.mceClusters.clusterKubeconfigs[name] = string(kubeconfig)
	m.mceClusters.clusterPasswords[name] = string(password)
	return string(kubeconfig), string(password), nil
}

func (m *jobManager) deleteManagedCluster(name string) error {
	if err := m.dpcrHiveClient.Delete(context.TODO(), &hivev1.ClusterDeployment{ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: name}}, &client.DeleteOptions{}); err != nil {
		return fmt.Errorf("failed to delete clusterdeployment object: %v", err)
	}
	if err := m.dpcrOcmClient.Delete(context.TODO(), &clusterv1.ManagedCluster{ObjectMeta: metav1.ObjectMeta{Name: name, Namespace: name}}, &client.DeleteOptions{}); err != nil {
		return fmt.Errorf("failed to delete managed cluster object: %v", err)
	}
	return nil
}
